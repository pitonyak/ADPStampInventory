## Script maps IP information exchanges to illustrate information flow
## and centrality metrics on the network. 
## Subnet option allows for filtering for traffic internal to a given subnet
## Output is an annotated GraphML file that can be imported into Cytoscape.
## Annotations include centrality calculations based on the completed graph 

## This is the only script that uses the .json file generated by the length_preprocess script

# Required Input: .json file from the length_preprocess script
# Optional Input: Number of clusters, output graphml name
# Output: GraphML file

import json
from argparse import ArgumentParser
from scapy.all import *
import numpy as np
from sklearn.cluster import KMeans, MiniBatchKMeans
import matplotlib.pyplot as plt

import time

def parse_length_dict(length_dict, conversation_id = None, index=0, verbose=False):

    if index == 0:
        reported_lengths = []
        if verbose:
            print(f"Returning only the reported lengths.")
    elif index == 1:
        wire_lengths = []
        if verbose:
            print(f"Returning only the wire lengths.")
    elif index == 2:
        reported_lengths = []
        wire_lengths = []
        if verbose:
            print(f"Returning a list each for reported length and wire length")
    else:
        print(f"Provide a valid index for the length lists.")
        return

    if conversation_id == None:
        print(f"Provide a conversation id to target")
        return

    for packet in length_dict[conversation_id]:
        if index == 0 or index == 2:
            reported_lengths.append(packet[0])
        if index == 1 or index == 2:
            wire_lengths.append(packet[1])

    if index == 0:
        return reported_lengths
    elif index == 1:
        return wire_lengths
    elif index == 2:
        return reported_lengths, wire_lengths

def main():
    parser = ArgumentParser()
    parser.add_argument('-f', '--file', help='Path to input json file', required=True)
    parser.add_argument('-c', '--clusters', help='Number of kmeans clusters to use', default=8)
    parser.add_argument('-d', '--directory', help='Directory of input json files')
    parser.add_argument('-o', '--output', help='File name for output graphml file', default= "output.graphml")
    args = parser.parse_args()

    with open(args.file, 'r') as f:
        print(f"Loading json file: {args.file}")
        length_dict = dict(json.load(f))
    
    packet_count = []
    X = []
    start = time.time()
    for key in length_dict:
        print(f"Loading conversation: {key}")
        reported_lengths = parse_length_dict(length_dict, conversation_id=key, index=0)
        X += reported_lengths
    finish = time.time()
    print(f"Loading all conversations took {finish - start:.3f} seconds.")

    X = np.array(X)
    start = time.time()    
    # kmeans = KMeans(n_clusters=int(args.clusters))
    kmeans = MiniBatchKMeans(n_clusters=int(args.clusters))
    kmeans.fit(X.reshape(-1,1))
    finish = time.time()
    print(f"Kmeans clustering took {finish - start:.3f} seconds.")
    # from IPython import embed; embed()

    plt.scatter(X, kmeans.labels_)
    # print(kmeans.cluster_centers_)
    # print(kmeans.labels_)
    plt.show()





if __name__ == '__main__':
    main()
